# -*- coding: utf-8 -*-
"""Tugas Crawling Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_2Bvg-Fjx8_YdQlScqCe4ZTs-kwRkxY

# Crawling data with Tweet Harvest
"""

#install library pandas dan node js pada g colab

!pip install pandas
!curl -sL https://deb.nodesource.com/setup_18.x | sudo -E bash -
!sudo apt-get install -y nodejs

#menyiapkan variabel untuk menyimpan file hasil crawling
#menentukan keyword

filename = 'ayamgeprek.csv'
search_keyword = 'ayam geprek'
limit = 100

#run tweet harvest

!npx --yes tweet-harvest@latest -o "{filename}" -s "{search_keyword}" -l {limit} --token "d0d7723a85829611db7957ec32e5f634e08cfe34"

import pandas as pd

#deklarasi file path
file_path = f"tweets-data/{filename}"

#deklarasi dataframe berdasarkan filepath tadi
df = pd.read_csv(file_path, delimiter=";")

display (df)

# Cek jumlah data

# Drop columns that are not needed (columns 3, 4, 5, 6, and 7)
df.drop(df.columns[[3, 4, 5, 6, 7]], axis=1, inplace=True)

# Display the modified DataFrame
display(df)

df.to_csv('ayamgeprek_dropped_column.csv', index=False)